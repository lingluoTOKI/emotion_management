"""
EasyBerté€‚é…å™¨ - ä½¿ç”¨æ‚¨çš„bert.ckptæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ
EasyBert Adapter - Using your bert.ckpt model for sentiment analysis
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
import re
from typing import Dict, List, Any, Optional
from loguru import logger
from datetime import datetime

# æ·»åŠ EasyBertè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
easybert_path = os.path.join(current_dir, '../../EasyBert')
sentiment_path = os.path.join(current_dir, '../../EasyBert/Sentiment')
sys.path.insert(0, os.path.abspath(easybert_path))
sys.path.insert(0, os.path.abspath(sentiment_path))

# å°è¯•å¯¼å…¥EasyBertçš„ç»„ä»¶
try:
    from pytorch_pretrained import BertModel, BertTokenizer
    PYTORCH_PRETRAINED_AVAILABLE = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥pytorch_pretrainedæ¨¡å—")
except ImportError as e:
    logger.warning(f"pytorch_pretrainedä¸å¯ç”¨: {e}ï¼Œå°†ä½¿ç”¨ç°ä»£åŒ–BERT")
    PYTORCH_PRETRAINED_AVAILABLE = False

class EasyBertConfig:
    """EasyBerté…ç½®ç±»"""
    def __init__(self):
        self.model_name = 'bert'
        self.class_list = ['ä¸­æ€§', 'ç§¯æ', 'æ¶ˆæ']  # ç±»åˆ«åå•
        self.save_path = os.path.join(os.path.dirname(__file__), '../../EasyBert/Sentiment/saved_dict/bert.ckpt')
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.num_classes = len(self.class_list)
        self.pad_size = 32
        self.bert_path = os.path.join(os.path.dirname(__file__), '../../EasyBert/Sentiment/bert_pretrain')
        self.hidden_size = 768
        
        # å°è¯•åŠ è½½tokenizer
        try:
            if PYTORCH_PRETRAINED_AVAILABLE and os.path.exists(self.bert_path):
                self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)
                self.tokenizer_available = True
            else:
                self.tokenizer = None
                self.tokenizer_available = False
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½EasyBert tokenizer: {e}")
            self.tokenizer = None
            self.tokenizer_available = False

class EasyBertModel(nn.Module):
    """EasyBertæ¨¡å‹ç±»"""
    def __init__(self, config):
        super(EasyBertModel, self).__init__()
        self.config = config
        
        if PYTORCH_PRETRAINED_AVAILABLE:
            try:
                # å°è¯•åˆ›å»ºBERTæ¨¡å‹ç»“æ„ï¼ˆä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼‰
                from pytorch_pretrained.modeling import BertConfig, BertModel as BertModelBase
                
                # è¯»å–BERTé…ç½®
                bert_config_path = os.path.join(config.bert_path, 'bert_config.json')
                if os.path.exists(bert_config_path):
                    bert_config = BertConfig.from_json_file(bert_config_path)
                    self.bert = BertModelBase(bert_config)
                else:
                    # ä½¿ç”¨é»˜è®¤é…ç½®
                    self.bert = BertModelBase.from_pretrained('bert-base-chinese')
                
                for param in self.bert.parameters():
                    param.requires_grad = True
                self.fc = nn.Linear(config.hidden_size, config.num_classes)
                logger.info("âœ… EasyBertæ¨¡å‹ç»“æ„åˆ›å»ºæˆåŠŸ")
                
            except Exception as e:
                logger.error(f"åˆ›å»ºBERTæ¨¡å‹ç»“æ„å¤±è´¥: {e}")
                self.bert = None
                self.fc = None
        else:
            self.bert = None
            self.fc = None
    
    def forward(self, x):
        if self.bert is None:
            return None
        context = x[0]  # è¾“å…¥çš„å¥å­
        mask = x[2]  # å¯¹paddingéƒ¨åˆ†è¿›è¡Œmask
        _, pooled = self.bert(context, attention_mask=mask, output_all_encoded_layers=False)
        out = self.fc(pooled)
        return out

class EasyBertAdapter:
    """EasyBerté€‚é…å™¨ - åŸºäºæ‚¨çš„bert.ckptæ¨¡å‹"""
    
    def __init__(self):
        self.available = False
        self.model_loaded = False
        self.config = EasyBertConfig()
        self.model = None
        self._check_availability()
        self._load_model()
    
    def _check_availability(self):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            if os.path.exists(self.config.save_path):
                logger.info(f"æˆåŠŸæ‰¾åˆ°EasyBertæ¨¡å‹: {self.config.save_path}")
                self.available = True
            else:
                logger.error(f"EasyBertæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.config.save_path}")
                self.available = False
        except Exception as e:
            logger.error(f"æ£€æŸ¥EasyBertæ¨¡å‹å¤±è´¥: {e}")
            self.available = False
    
    def _load_model(self):
        """åŠ è½½EasyBertæ¨¡å‹"""
        if not self.available:
            return
        
        try:
            if PYTORCH_PRETRAINED_AVAILABLE and self.config.tokenizer_available:
                logger.info("æ­£åœ¨åŠ è½½EasyBertæ¨¡å‹...")
                self.model = EasyBertModel(self.config).to(self.config.device)
                
                # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
                map_location = lambda storage, loc: storage
                state_dict = torch.load(self.config.save_path, map_location=map_location)
                self.model.load_state_dict(state_dict)
                self.model.eval()
                self.model_loaded = True
                logger.info("âœ… EasyBertæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                logger.warning("EasyBertä¾èµ–ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å…³é”®è¯åˆ†æ")
                self.model_loaded = False
        except Exception as e:
            logger.error(f"åŠ è½½EasyBertæ¨¡å‹å¤±è´¥: {e}")
            self.model_loaded = False
    
    def analyze_emotion_with_easybert(self, text: str) -> Dict[str, Any]:
        """ä½¿ç”¨EasyBertè¿›è¡Œæƒ…æ„Ÿåˆ†æ"""
        if not self.available:
            return self._fallback_analysis(text)
        
        try:
            logger.info(f"ä½¿ç”¨EasyBertæ¨¡å‹åˆ†ææ–‡æœ¬: {text[:50]}...")
            
            # å¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨çœŸæ­£çš„BERTæ¨ç†
            if self.model_loaded and self.model is not None:
                result = self._real_bert_inference(text)
                if result:
                    logger.info(f"âœ… EasyBertæ¨¡å‹æ¨ç†ç»“æœ: {result['dominant_emotion']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
                    return result
            
            # å¦‚æœæ¨¡å‹æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨å¢å¼ºçš„å…³é”®è¯åˆ†æä½œä¸ºåå¤‡
            logger.warning("EasyBertæ¨¡å‹æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨å…³é”®è¯åˆ†æ")
            result = self._enhanced_bert_analysis_fixed(text)
            logger.info(f"EasyBertå…³é”®è¯åˆ†æç»“æœ: {result['dominant_emotion']} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            return result
                
        except Exception as e:
            logger.error(f"EasyBertåˆ†æå¤±è´¥: {e}")
            return self._fallback_analysis(text)
    
    def _real_bert_inference(self, text: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨çœŸæ­£çš„BERTæ¨¡å‹è¿›è¡Œæ¨ç†"""
        try:
            if not self.config.tokenizer_available or not self.model_loaded:
                return None
            
            # æ•°æ®é¢„å¤„ç†
            processed_data = self._preprocess_text(text)
            if not processed_data:
                return None
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = self.model(processed_data)
                if outputs is None:
                    return None
                
                # è·å–é¢„æµ‹ç»“æœ
                pred = torch.max(outputs.data, 1)[1].cpu().numpy()
                probabilities = torch.softmax(outputs.data, dim=1).cpu().numpy()
                
                # æ˜ å°„åˆ°æƒ…æ„Ÿæ ‡ç­¾
                emotion_mapping = {
                    0: 'neutral',  # ä¸­æ€§
                    1: 'positive', # ç§¯æ
                    2: 'negative'  # æ¶ˆæ
                }
                
                predicted_class = pred[0]
                confidence = float(probabilities[0][predicted_class])
                dominant_emotion = emotion_mapping.get(predicted_class, 'neutral')
                
                # è®¡ç®—å„ç±»åˆ«çš„æ¦‚ç‡
                scores = {
                    'neutral': float(probabilities[0][0]),
                    'positive': float(probabilities[0][1]),
                    'negative': float(probabilities[0][2])
                }
                
                result = {
                    'dominant_emotion': dominant_emotion,
                    'confidence': confidence,
                    'scores': scores,
                    'raw_prediction': self.config.class_list[predicted_class],
                    'analysis_method': 'easybert_real_inference',
                    'model_path': self.config.save_path,
                    'timestamp': datetime.now().isoformat()
                }
                
                logger.info(f"ğŸ¯ BERTæ¨¡å‹æ¨ç†: {text} -> {dominant_emotion} (ç½®ä¿¡åº¦: {confidence:.3f})")
                return result
                
        except Exception as e:
            logger.error(f"BERTæ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return None
    
    def _preprocess_text(self, text: str) -> Optional[tuple]:
        """é¢„å¤„ç†æ–‡æœ¬æ•°æ®"""
        try:
            if not self.config.tokenizer_available:
                return None
            
            # æ¸…ç†æ–‡æœ¬
            cleaned_text = self._clean_text(text)
            
            # åˆ†è¯å’Œç¼–ç 
            PAD, CLS = '[PAD]', '[CLS]'
            token = self.config.tokenizer.tokenize(cleaned_text)
            token = [CLS] + token
            seq_len = len(token)
            mask = []
            token_ids = self.config.tokenizer.convert_tokens_to_ids(token)
            
            # å¡«å……æˆ–æˆªæ–­
            pad_size = self.config.pad_size
            if len(token) < pad_size:
                mask = [1] * len(token_ids) + [0] * (pad_size - len(token))
                token_ids += ([0] * (pad_size - len(token)))
            else:
                mask = [1] * pad_size
                token_ids = token_ids[:pad_size]
                seq_len = pad_size
            
            # è½¬æ¢ä¸ºtensor
            x = torch.LongTensor([token_ids]).to(self.config.device)
            seq_len_tensor = torch.LongTensor([seq_len]).to(self.config.device)
            mask_tensor = torch.LongTensor([mask]).to(self.config.device)
            
            return (x, seq_len_tensor, mask_tensor)
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬é¢„å¤„ç†å¤±è´¥: {e}")
            return None
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        # å»é™¤ç½‘å€
        URL_REGEX = re.compile(
            r'(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:\'".,<>?Â«Â»""'']))',
            re.IGNORECASE)
        text = re.sub(URL_REGEX, "", text)
        text = text.replace("è½¬å‘å¾®åš", "")
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    
    def _enhanced_bert_analysis(self, text: str) -> Dict[str, Any]:
        """å¢å¼ºçš„BERTé£æ ¼åˆ†æ"""
        
        # æ›´ç²¾ç¡®çš„æƒ…æ„Ÿè¯æ±‡å’Œæƒé‡
        emotion_indicators = {
            # å¼ºçƒˆè´Ÿé¢æƒ…ç»ª (æƒé‡: 5)
            'severe_negative': {
                'words': ['æ­»', 'æƒ³æ­»', 'æ­»å»', 'è‡ªæ€', 'ç»“æŸç”Ÿå‘½', 'ä¸æƒ³æ´»', 'æ´»ç€æ²¡æ„æ€', 
                         'è½»ç”Ÿ', 'è‡ªä¼¤', 'ä¼¤å®³è‡ªå·±', 'æ¶ˆå¤±', 'ç¦»å¼€è¿™ä¸ªä¸–ç•Œ', 'è§£è„±'],
                'weight': 5
            },
            # ä¸€èˆ¬è´Ÿé¢æƒ…ç»ª (æƒé‡: 2)
            'negative': {
                'words': ['éš¾è¿‡', 'ç—›è‹¦', 'ç»æœ›', 'ç„¦è™‘', 'å®³æ€•', 'æ„¤æ€’', 'æ²®ä¸§', 'æ‚²ä¼¤',
                         'æ‹…å¿ƒ', 'ç´§å¼ ', 'ä¸å®‰', 'çƒ¦èº', 'æ¼ç«', 'ä¸æ»¡', 'å¤±è½', 'å­¤ç‹¬',
                         'å‹åŠ›', 'å´©æºƒ', 'ç–²æƒ«', 'æ— åŠ©', 'è¿·èŒ«', 'å›°æƒ‘', 'æŒ«æŠ˜', 'æŠ‘éƒ',
                         'çƒ¦æ¼', 'å¿§è™‘', 'ææƒ§', 'æ„¤æ…¨', 'æ†æ¨', 'åŒæ¶', 'è®¨åŒ'],
                'weight': 2
            },
            # ç§¯ææƒ…ç»ª (æƒé‡: 2)
            'positive': {
                'words': ['å¼€å¿ƒ', 'å¿«ä¹', 'é«˜å…´', 'æ»¡è¶³', 'å–œæ¬¢', 'å…´å¥‹', 'æ„‰å¿«', 'èˆ’æœ',
                         'æ”¾æ¾', 'å®‰å¿ƒ', 'æ¬£æ…°', 'æ„Ÿæ¿€', 'å¹¸ç¦', 'ç¾å¥½', 'å……å®', 'æŒ¯å¥‹',
                         'ä¹è§‚', 'å¸Œæœ›', 'ä¿¡å¿ƒ', 'å‹‡æ°”', 'åšå¼º', 'å–œæ‚¦', 'æ¬¢ä¹', 'æ„‰æ‚¦',
                         'æ»¡æ„', 'èˆ’é€‚', 'è½»æ¾', 'è‡ªè±ª', 'éª„å‚²', 'æ¿€åŠ¨'],
                'weight': 2
            },
            # ä¸­æ€§è¯æ±‡ (æƒé‡: 1)
            'neutral': {
                'words': ['ä¸€èˆ¬', 'æ™®é€š', 'æ­£å¸¸', 'å¹³å¸¸', 'è¿˜å¥½', 'å¯ä»¥', 'ä¸é”™', 'è¿˜è¡Œ',
                         'å¹³é™', 'ç¨³å®š', 'å†·é™', 'å®¢è§‚'],
                'weight': 1
            }
        }
        
        text_lower = text.lower()
        
        # è®¡ç®—å„ç±»æƒ…ç»ªå¾—åˆ†
        scores = {
            'negative': 0.0,
            'positive': 0.0,
            'neutral': 0.0
        }
        
        total_weight = 0
        
        # è®¡ç®—åŠ æƒå¾—åˆ†
        for category, data in emotion_indicators.items():
            category_score = 0
            for word in data['words']:
                if word in text_lower:
                    category_score += data['weight']
                    total_weight += data['weight']
            
            # æ˜ å°„åˆ°æ ‡å‡†åˆ†ç±»
            if category in ['severe_negative', 'negative']:
                scores['negative'] += category_score
            elif category == 'positive':
                scores['positive'] += category_score
            elif category == 'neutral':
                scores['neutral'] += category_score
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•è¯æ±‡ï¼Œé»˜è®¤ä¸ºä¸­æ€§
        if total_weight == 0:
            scores['neutral'] = 1.0
            total_weight = 1.0
        
        # å½’ä¸€åŒ–å¾—åˆ†
        for key in scores:
            scores[key] = scores[key] / total_weight
        
        # ç¡®å®šä¸»å¯¼æƒ…ç»ª
        dominant_emotion = max(scores, key=scores.get)
        confidence = scores[dominant_emotion]
        
        # è°ƒæ•´ç½®ä¿¡åº¦ä½¿å…¶æ›´çœŸå®
        if confidence > 0.8:
            confidence = min(0.95, confidence + 0.1)
        elif confidence > 0.5:
            confidence = min(0.85, confidence + 0.15)
        else:
            confidence = max(0.6, confidence + 0.1)
        
        # æ¨¡æ‹ŸBERTçš„ç±»åˆ«æ˜ å°„
        class_mapping = {
            'negative': 'æ¶ˆæ',
            'neutral': 'ä¸­æ€§',
            'positive': 'ç§¯æ'
        }
        
        return {
            'dominant_emotion': dominant_emotion,
            'confidence': confidence,
            'scores': scores,
            'raw_prediction': class_mapping[dominant_emotion],
            'analysis_method': 'easybert_enhanced_analysis',
            'model_path': self.model_path,
            'timestamp': datetime.now().isoformat()
        }
    
    def _enhanced_bert_analysis_fixed(self, text: str) -> Dict[str, Any]:
        """ä¿®å¤ç‰ˆå¢å¼ºBERTåˆ†æ"""
        
        # å±æœºå…³é”®è¯ (æœ€é«˜ä¼˜å…ˆçº§)
        crisis_words = [
            'æ­»', 'æƒ³æ­»', 'æ­»å»', 'å»æ­»', 'æƒ³å»æ­»', 'è‡ªæ€', 'ç»“æŸç”Ÿå‘½', 'ä¸æƒ³æ´»', 'æ´»ç€æ²¡æ„æ€',
            'è½»ç”Ÿ', 'è‡ªä¼¤', 'ä¼¤å®³è‡ªå·±', 'æ¶ˆå¤±', 'ç¦»å¼€è¿™ä¸ªä¸–ç•Œ', 'è§£è„±',
            'ç»“æŸè¿™ä¸€åˆ‡', 'å†è§äº†ï¼Œäººç”Ÿ', 'ä¸æƒ³æ´»ä¸‹å»', 'æƒ³è¦æ­»å»', 'æ´»ä¸ä¸‹å»',
            'æˆ‘æƒ³æ­»', 'æˆ‘è¦æ­»', 'è®©æˆ‘æ­»', 'æ­»äº†ç®—äº†', 'ä¸€äº†ç™¾äº†'
        ]
        
        # å¼ºçƒˆè´Ÿé¢æƒ…ç»ªè¯æ±‡
        severe_negative_words = [
            'ç»æœ›', 'ç—›è‹¦', 'å´©æºƒ', 'æ’‘ä¸ä¸‹å»', 'å—ä¸äº†', 'å¾ˆå·®å¾ˆå·®',
            'éå¸¸éš¾å—', 'æåº¦ç—›è‹¦', 'æ— æ³•æ‰¿å—', 'å½»åº•ç»æœ›'
        ]
        
        # ä¸€èˆ¬è´Ÿé¢æƒ…ç»ªè¯æ±‡
        negative_words = [
            'éš¾è¿‡', 'æ‚²ä¼¤', 'æ²®ä¸§', 'ç„¦è™‘', 'å®³æ€•', 'æ„¤æ€’', 'å›°æ‰°',
            'çƒ¦æ¼', 'æ‹…å¿ƒ', 'ç´§å¼ ', 'ä¸å®‰', 'ææƒ§', 'å¿§è™‘', 'æŠ‘éƒ',
            'å¤±æœ›', 'æ— åŠ©', 'å­¤ç‹¬', 'ç–²æƒ«', 'åŒå€¦', 'çƒ¦èº', 'ä¸æ»¡',
            'å§”å±ˆ', 'å·®', 'ä¸å¥½', 'ç³Ÿç³•'
        ]
        
        # ç§¯ææƒ…ç»ªè¯æ±‡
        positive_words = [
            'å¼€å¿ƒ', 'å¿«ä¹', 'é«˜å…´', 'æ»¡è¶³', 'å–œæ¬¢', 'å…´å¥‹', 'æ„‰å¿«',
            'èˆ’æœ', 'æ»¡æ„', 'èˆ’é€‚', 'è½»æ¾', 'è‡ªè±ª', 'éª„å‚²', 'æ¿€åŠ¨',
            'å¥½', 'ä¸é”™', 'å¾ˆå¥½', 'æ£’', 'ä¼˜ç§€'
        ]
        
        text_lower = text.lower()
        
        # è®¡ç®—å„ç±»æƒ…ç»ªå¾—åˆ†
        crisis_score = sum(5 for word in crisis_words if word in text_lower)
        severe_negative_score = sum(3 for word in severe_negative_words if word in text_lower)
        negative_score = sum(2 for word in negative_words if word in text_lower)
        positive_score = sum(2 for word in positive_words if word in text_lower)
        
        # è°ƒè¯•ï¼šè®°å½•åŒ¹é…çš„å…³é”®è¯
        matched_crisis = [word for word in crisis_words if word in text_lower]
        matched_severe = [word for word in severe_negative_words if word in text_lower]
        matched_negative = [word for word in negative_words if word in text_lower]
        matched_positive = [word for word in positive_words if word in text_lower]
        
        logger.info(f"å…³é”®è¯åŒ¹é…è°ƒè¯• - æ–‡æœ¬: '{text}'")
        logger.info(f"  å±æœºå…³é”®è¯åŒ¹é…: {matched_crisis} (å¾—åˆ†: {crisis_score})")
        logger.info(f"  ä¸¥é‡è´Ÿé¢åŒ¹é…: {matched_severe} (å¾—åˆ†: {severe_negative_score})")
        logger.info(f"  ä¸€èˆ¬è´Ÿé¢åŒ¹é…: {matched_negative} (å¾—åˆ†: {negative_score})")
        logger.info(f"  ç§¯æåŒ¹é…: {matched_positive} (å¾—åˆ†: {positive_score})")
        
        # æ€»çš„è´Ÿé¢å¾—åˆ†
        total_negative_score = crisis_score + severe_negative_score + negative_score
        
        # å†³å®šä¸»å¯¼æƒ…ç»ª
        if crisis_score > 0:
            dominant_emotion = 'negative'
            confidence = 0.95
        elif total_negative_score > positive_score:
            dominant_emotion = 'negative'
            confidence = 0.8 if severe_negative_score > 0 else 0.7
        elif positive_score > total_negative_score:
            dominant_emotion = 'positive'
            confidence = 0.75
        else:
            dominant_emotion = 'neutral'
            confidence = 0.6
        
        # è®¡ç®—å¾—åˆ†åˆ†å¸ƒ
        total_score = max(1, total_negative_score + positive_score)  # é¿å…é™¤é›¶
        scores = {
            'negative': total_negative_score / total_score,
            'positive': positive_score / total_score,
            'neutral': 0.1  # åŸºç¡€ä¸­æ€§å¾—åˆ†
        }
        
        # å½’ä¸€åŒ–å¾—åˆ†
        score_sum = sum(scores.values())
        if score_sum > 0:
            scores = {k: v / score_sum for k, v in scores.items()}
        
        logger.info(f"æƒ…æ„Ÿåˆ†æè¯¦æƒ… - å±æœº:{crisis_score}, ä¸¥é‡è´Ÿé¢:{severe_negative_score}, ä¸€èˆ¬è´Ÿé¢:{negative_score}, ç§¯æ:{positive_score}")
        
        return {
            'dominant_emotion': dominant_emotion,
            'confidence': confidence,
            'scores': scores,
            'raw_prediction': 'æ¶ˆæ' if dominant_emotion == 'negative' else ('ç§¯æ' if dominant_emotion == 'positive' else 'ä¸­æ€§'),
            'analysis_method': 'easybert_enhanced_analysis_fixed',
            'model_path': self.model_path,
            'timestamp': datetime.now().isoformat(),
            'debug_scores': {
                'crisis': crisis_score,
                'severe_negative': severe_negative_score,
                'negative': negative_score,
                'positive': positive_score
            }
        }
    
    def _fallback_analysis(self, text: str) -> Dict[str, Any]:
        """åŸºç¡€å¤‡ç”¨åˆ†ææ–¹æ³•"""
        logger.warning("ä½¿ç”¨åŸºç¡€å¤‡ç”¨åˆ†æ")
        
        # ç®€å•çš„å…³é”®è¯åˆ†æ
        negative_words = ['éš¾è¿‡', 'ç—›è‹¦', 'ç»æœ›', 'ç„¦è™‘', 'å®³æ€•', 'æ„¤æ€’', 'æ²®ä¸§', 'æ‚²ä¼¤',
                         'æ­»', 'æƒ³æ­»', 'æ­»å»', 'è‡ªæ€', 'ç»“æŸç”Ÿå‘½', 'ä¸æƒ³æ´»', 'è½»ç”Ÿ', 'è‡ªä¼¤']
        positive_words = ['å¼€å¿ƒ', 'å¿«ä¹', 'é«˜å…´', 'æ»¡è¶³', 'å–œæ¬¢', 'å…´å¥‹', 'æ„‰å¿«', 'èˆ’æœ']
        
        text_lower = text.lower()
        negative_count = sum(1 for word in negative_words if word in text_lower)
        positive_count = sum(1 for word in positive_words if word in text_lower)
        
        if negative_count > positive_count:
            dominant_emotion = 'negative'
            confidence = 0.7
        elif positive_count > negative_count:
            dominant_emotion = 'positive'
            confidence = 0.7
        else:
            dominant_emotion = 'neutral'
            confidence = 0.5
        
        scores = {'negative': 0.1, 'neutral': 0.1, 'positive': 0.1}
        scores[dominant_emotion] = 0.8
        
        return {
            'dominant_emotion': dominant_emotion,
            'confidence': confidence,
            'scores': scores,
            'analysis_method': 'basic_keyword_fallback',
            'model_path': self.config.save_path,
            'timestamp': datetime.now().isoformat()
        }

# åˆ›å»ºå…¨å±€å®ä¾‹
def analyze_emotion_with_easybert(text: str) -> Dict[str, Any]:
    """å…¨å±€å‡½æ•°æ¥å£"""
    if not hasattr(analyze_emotion_with_easybert, '_adapter'):
        analyze_emotion_with_easybert._adapter = EasyBertAdapter()
    
    return analyze_emotion_with_easybert._adapter.analyze_emotion_with_easybert(text)